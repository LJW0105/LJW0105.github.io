---
# Feel free to add content and custom Front Matter to this file.
# To modify the layout, see https://jekyllrb.com/docs/themes/#overriding-theme-defaults

layout: default
---
Hi, I'm Jingwen, a PhD student in the [Theory of Computation Group](https://theory.cs.columbia.edu/) at [Columbia University](https://www.columbia.edu/), where I am extremely fortunate to be advised by [Daniel Hsu](https://www.cs.columbia.edu/~djhsu/) and [Alex Andoni](https://www.cs.columbia.edu/~andoni/). I obtained my undergraduate degree at UC San Diego, where I majored in mathematics and computer science. During my time at UCSD, I'm extremely honored to work with glorious [Sanjoy Dasgupta](https://cseweb.ucsd.edu/~dasgupta/) and [Russell Impagliazzo](https://cseweb.ucsd.edu/~russell/), who sparked my interest in ML theory and TCS research. I was also very lucky to work with [Xiaolong Wang](https://xiaolonw.github.io/), who introduced me to CS research. 

My research interest lies broadly in machine learning theory, algorithmic statistics, and deep learning theory. Specifically, I'm curious about the theory of transformers, particularly understanding what kinds of algorithms they can implement and efficiently learn, exploring how to train them or design training data to help them learn correct algorithms, and developing more efficient alternatives that retain their representational power. I'm also interested in designing computationally and sample-efficient algorithms that can adapt to the low-dimensional structure of the data, for tasks like learning [multi-index models](https://arxiv.org/pdf/2504.05426) or more general non-linear feature learning settings. 


## Experience
- Computer and Information Science at the University of Pennsylvania (Summer 2025), hosted by the amazing [Surbhi Goel](https://www.surbhigoel.com/)
- Simons Institute for the Theory of Computing, UC Berkeley (Fall 2024), [Modern Paradigms in Generalization](https://simons.berkeley.edu/programs/modern-paradigms-generalization) and [Large Language Models and Transformers](https://simons.berkeley.edu/programs/special-year-large-language-models-transformers-part-1)

## Publications
[Group-realizable multi-group learning by minimizing empirical risk]()<br>
($\alpha$-$\beta$) Navid Ardeshir, Samuel Deng, Daniel Hsu, Jingwen Liu. <br>
*The 37th International Conference on Algorithmic Learning Theory (ALT),* 2026. <br>
[arXiv](https://www.arxiv.org/abs/2601.16922)

[Fast attention mechanisms: a tale of parallelism]()<br>
Jingwen Liu, Hantao Yu, Clayton Sanford, Alexandr Andoni, Daniel Hsu. <br>
*Advances in Neural Information Processing Systems (NeurIPS),* 2025. <br>
[arXiv](https://arxiv.org/abs/2601.16922) | [Poster](/assets/poster_fast_attention.pdf)

[Group-wise oracle-efficient algorithms for online multi-group learning](https://proceedings.neurips.cc/paper_files/paper/2024/file/45afdc1958befe9b60af7b445e768b10-Paper-Conference.pdf)<br>
(random order) Samuel Deng, Daniel Hsu, Jingwen Liu.<br>
*Advances in Neural Information Processing Systems (NeurIPS),* 2024. <br>
[arXiv](https://arxiv.org/abs/2406.05287) | [Poster](/assets/poster_oracle.pdf) 

[VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_VideoINR_Learning_Video_Implicit_Neural_Representation_for_Continuous_Space-Time_Super-Resolution_CVPR_2022_paper.pdf)<br>
Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey Shi, Xiaolong Wang.<br>
*Conference on Computer Vision and Pattern Recognition (CVPR),* 2022. <br>
[arXiv](https://arxiv.org/abs/2206.04647) | [website](https://zeyuan-chen.com/VideoINR/)


## Teaching and Service

I have served as a reviewer for: COLT 2025, ICML 2025, NeurIPS 2024, 2025. 

I was also a volunteer for [Pre-Submission Application Review (PAR) Program](https://www.cs.columbia.edu/cscu-phd-par-program/) which provides reviews of statements and CV from students applying to the CS PhD Program at Columbia. 

I was a teaching assistant for the following classes:
- Theoretical Foundations of LLMs, by Daniel Hsu, spring 2025, Columbia University
- Machine Learning Theory, by Daniel Hsu, spring 2024, Columbia University
- Machine Learning, by Sanjoy Dasgupta, winter 2023, UCSD
- Mathematics for Algorithms and Systems, by Russell Impagliazzo, fall 2022, UCSD
- Software Project for Computer Architecture, by Hung-Wei Tseng, summer 2022, UCSD
- Introduction to Programming, by Gerald Soosairaj, spring 2022, UCSD

I was a grader for the following classes:
- Abstract Algebra, fall 2021, UCSD
- Differential Equations, spring 2021, UCSD
- Mathematical Reasoning, winter 2021, UCSD

## Miscellaneous

I am deeply grateful to my advisors and many others in the community for their great mentorship, including [Bingbin Liu](https://clarabing.github.io/), [Surbhi Goel](https://www.surbhigoel.com/), [Samuel Deng](https://samuel-deng.github.io/), [Yuhao Li](https://yuhao.li/), [Hantao Yu](https://www.hantaoyu.org/), [Clayton Sanford](https://claytonsanford.com/), [Navid Ardeshir](https://mathblasphemy.netlify.app/), [Samantha Chen](https://chens5.github.io/), [Geelon So](https://geelon.github.io/), [Ezra Edelman](https://www.ezraedelman.com/), [Sihan Liu](https://lteins.github.io/) and this list **goes on and on**... I would like to pass on this culture, so feel free to reach out to me if you think I might be helpful!

I like playing ping pong, climbing, skiing and karaoke :)

Website template follows the tradition of Daniel's students [Geelon So](https://geelon.github.io/) and [Samuel Deng](https://samuel-deng.github.io/).
